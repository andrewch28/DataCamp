{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assesments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# Continue is used to skip an iteration\n",
    "for i in \"hello\":\n",
    "    if i == \"l\":\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.rename(columns={\"A\": \"a\", \"B\": \"c\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy allows us to rapidly carry out operations over entire collection of values (it is impossible to divide one list by another, however you CAN do it with numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numpy allows to perform calculations over entire arrays\n",
    "import numpy as np\n",
    "\n",
    "height = [1.73, 1.68, 1.71, 1.79]\n",
    "np_height = np.array(height)\n",
    "#numpy array takes list as an input\n",
    "\n",
    "weight = [64.5, 88.4, 68.7, 59.2]\n",
    "np_weight = np.array(weight)\n",
    "\n",
    "bmi = np_weight / np_height ** 2\n",
    "bmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that numpy can only contain values of a single type \n",
    "\n",
    "\n",
    "https://github.com/andrewch28/DataCamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi > 23 # This will return a boolean array\n",
    "\n",
    "#If we want to subset bmi array to only those elements, which are higher than 23:\n",
    "bmi[bmi>23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape attribute\n",
    "\n",
    "np_2d = np.array([[64.5, 88.4, 68.7, 59.2],\n",
    "                   [1.73, 1.68, 1.71, 1.79]])\n",
    "np_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_2d[0, 2])\n",
    "np_2d[:, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np_2d[0]))\n",
    "print(np.median(np_2d[0]))\n",
    "\n",
    "print(np.corrcoef(np_2d[0], np_2d[1]))\n",
    "print(np.std(np_2d[0]))\n",
    "\n",
    "print(np.sum(np_2d[0]))\n",
    "print(np.sort(np_2d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert positions and heights to numpy arrays: np_positions, np_heights\n",
    "positions = [\"GK\", ...]\n",
    "heights = [...]\n",
    "\n",
    "np_positions = np.array(positions)\n",
    "np_heights = np.array(heights)\n",
    "\n",
    "# Heights of the goalkeepers: gk_heights\n",
    "gk_heights = np_heights[np_positions == \"GK\"]\n",
    "\n",
    "# Heights of the other players: other_heights\n",
    "other_heights = np_heights[np_positions != \"GK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Pyton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams[\"figure.figsize\"]=(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lineplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "year = [1950, 1970, 1990, 2010]\n",
    "pop = [2.519, 3.692, 5.263, 6.972]\n",
    "plt.plot(year, pop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "year = [1950, 1970, 1990, 2010]\n",
    "pop = [2.519, 3.692, 5.263, 6.972]\n",
    "plt.scatter(year, pop)\n",
    "plt.show()\n",
    "#plt.savefig('filename.png', dpi=300)# - to save the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a time scale along horizontal axis - the line plot will be our friend, however if we are trying to establish a correlation between two variables it is a better practice to use scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the x-axis on a logarithmic scale\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram - to understand the distribution of our data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values = [0.6,1,5,6,7,4,8.3,5,4,3]\n",
    "plt.hist(values, bins=3) #bins are set to 10 by default\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resolution of our graph is controled by the dpi, so we should make it higher when we are saving our graph\n",
    "```plt.savefig('filename.png', dpi=300)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show() #displays a plot\n",
    "\n",
    "plt.clf() #cleans it up again so you can start afresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen that lineplot is best suited when we have dates on the x axis, scatter plot is the best option to seek correlation between two variables and histogram for understanding the distribution of our data (distribution of grades in class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "year = [1950, 1970, 1990, 2010]\n",
    "pop = [2.519, 3.692, 5.263, 6.972]\n",
    "\n",
    "plt.plot(year, pop)\n",
    "\n",
    "plt.title(\"World Population Projections\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Population\")\n",
    "plt.yticks([0,2,4,6,8,10], ['0', '2B', '4B', '6B', '8B', '10B']) #the first list are the actual values \n",
    "#and the second one are the values displayed at the graph\n",
    "#plt.savefig('filename.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et cetera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also have parameters for size, color and visibility of our data (or dots)\n",
    "gdp_cap=[...]\n",
    "life_exp=[...]\n",
    "colors=[...]\n",
    "\n",
    "plt.scatter(gdp_cap, life_exp, s = pop, c = colors, alphs = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add a text by specifying its coordinates and then printing the text itself\n",
    "plt.text(1550, 71, 'India')\n",
    "plt.text(5700, 80, 'China')\n",
    "\n",
    "# Add grid()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_pop = np.array(pop)\n",
    "np_pop = np_pop * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick reminder about index method\n",
    "\n",
    "countries = ['spain', 'france', 'germany', 'norway']\n",
    "capitals = ['madrid', 'paris', 'berlin', 'oslo']\n",
    "\n",
    "# Get index of 'germany': ind_ger\n",
    "ind_ger = countries.index('germany')\n",
    "# Use ind_ger to print out capital of Germany\n",
    "print(capitals[ind_ger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n",
    "\n",
    "print(europe.keys())\n",
    "# The keys() method returns a view object. The view object contains the keys of the dictionary, as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keys must be unique, they also have to be immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = {}\n",
    "#To add/change key-value pair in a dictionary\n",
    "world[\"sealand\"] = 0.000028\n",
    "\n",
    "#To remove it\n",
    "del(world[\"sealand\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #built in numpy\n",
    "\n",
    "#We can create a DataFrame from a dictionary\n",
    "brics = pd.DataFrame(dict)\n",
    "brics.index = [\"BR\", \"RU\", \"IN\", \"CH\", \"SA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>capital</th>\n",
       "      <th>area</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BR</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Brasilia</td>\n",
       "      <td>8.516</td>\n",
       "      <td>200.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RU</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>17.100</td>\n",
       "      <td>143.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>India</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>3.286</td>\n",
       "      <td>1252.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH</th>\n",
       "      <td>China</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>9.597</td>\n",
       "      <td>1357.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SA</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>1.221</td>\n",
       "      <td>52.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country    capital    area  population\n",
       "BR        Brazil   Brasilia   8.516      200.40\n",
       "RU        Russia     Moscow  17.100      143.50\n",
       "IN         India  New Delhi   3.286     1252.00\n",
       "CH         China    Beijing   9.597     1357.00\n",
       "SA  South Africa   Pretoria   1.221       52.98"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "brics=pd.read_csv(\"brics.csv\", index_col=0)\n",
    "#display(brics)\n",
    "brics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat GPT have created a better visualization for pandas Series for me - new_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def new_display(series):\n",
    "    # Determine the width of each column based on the longest content\n",
    "    max_index_length = max(len(str(idx)) for idx in series.index)\n",
    "    max_value_length = max(len(str(val)) for val in series.values)\n",
    "    max_name_length = len(series.name)\n",
    "    \n",
    "    # Ensure the header fits the longest content\n",
    "    header_index = max(max_index_length, len(\"Index\"))\n",
    "    header_value = max(max_value_length, max_name_length)\n",
    "    \n",
    "    # Create the header\n",
    "    header = f\"| {'Index'.ljust(header_index)} | {series.name.ljust(header_value)} |\"\n",
    "    separator = f\"|{'-' * (header_index + 2)}|{'-' * (header_value + 2)}|\"\n",
    "    \n",
    "    # Create the rows\n",
    "    rows = \"\\n\".join([f\"| {str(idx).ljust(header_index)} | {str(val).ljust(header_value)} |\" for idx, val in series.items()])\n",
    "    \n",
    "    # Display the Markdown table\n",
    "    display(Markdown(f\"{header}\\n{separator}\\n{rows}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By using single square brackets we transform a DataFrame into a pandas Series\n",
    "new_display(brics['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#However if we use double square brackets it will remain a DataFrame\n",
    "#P.S. that way we don't have to use GPT's function)\n",
    "brics[['country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get row we use numbers like:\n",
    "brics[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc (label) and iloc (position) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting rows\n",
    "brics.loc[[\"RU\"]] #we use double square brackets to get a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting rows and columns\n",
    "brics.loc[[\"RU\", \"IN\", \"CH\"],[\"country\", \"capital\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = \"RU\"\n",
    "column=\"drives_right\"\n",
    "\n",
    "#To return a Series\n",
    "brics.loc[row, column]\n",
    "\n",
    "#To return a DF:\n",
    "brics.loc[[row], [column]]\n",
    "\n",
    "#Same applies to an iloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic, Control Flow and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True corresponds to 1 and False to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "my_house = np.array([18.0, 20.0, 10.75, 9.50])\n",
    "\n",
    "my_house >= 18\n",
    "#Behind the scenes numpy creates an array of [18, 18, 18, 18] and compares it\n",
    "#returns a list of booleans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three booleans operators:\n",
    "- and\n",
    "- or\n",
    "- not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy:\n",
    "- np.logical_and()\n",
    "- np.logical_or()\n",
    "- np.logical_not()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As soon as any given condition is successfull programm will stop\n",
    "# In this case only 2 will be printed out and 3 will never be reached\n",
    "x = 6\n",
    "if x % 2 == 0:\n",
    "    print(\"2\")\n",
    "elif x % 3 == 0:\n",
    "    print(\"3\")\n",
    "else:\n",
    "    print(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics[brics['area'] > 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics[(brics['area'] > 8) & (brics['area'] < 10)]\n",
    "\n",
    "#import numpy as np\n",
    "#brics[np.logical_and(brics[\"area\"] > 8, brics[\"area\"] < 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new on the while loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using enumerate\n",
    "fam = [1.73, 1.68, 1.71, 1.89]\n",
    "\n",
    "for index, value in enumerate(fam):\n",
    "    print(f\"index {index}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict.items() method\n",
    "\n",
    "world = {\n",
    "    \"afganistan\":30.55,\n",
    "    \"albania\":2.77,\n",
    "    \"algeria\":39.21,\n",
    "}\n",
    "\n",
    "for key, value in world.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy np.nditer() function for 2D arrays\n",
    "import numpy as np\n",
    "\n",
    "np_height = np.array([1.73, 1.68, 1.89, 1.79])\n",
    "np_weight = np.array([65.4, 59.2, 88.4, 68.7])\n",
    "meas = np.array([np_height, np_weight])\n",
    "\n",
    "for item in np.nditer(meas):\n",
    "    print(item)\n",
    "\n",
    "# We don't have to this use for 1D nupy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "brics = pd.read_csv(\"brics.csv\", index_col=0)\n",
    "\n",
    "for label, row in brics.iterrows():\n",
    "    print(label)\n",
    "    print(row)\n",
    "    #we can tap into a value using row.population for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, row in brics.iterrows():\n",
    "    brics.loc[label, \"name_length\"] = len(row[\"country\"])\n",
    "brics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function\n",
    "import pandas as pd\n",
    "brics = pd.read_csv(\"brics.csv\", index_col=0)\n",
    "#To apply a function to each column element\n",
    "brics[\"length\"] = brics[\"country\"].apply(len)\n",
    "brics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .apply(str.upper)\n",
    "brics[\"COUNTRY\"] = brics[\"country\"].apply(str.upper)\n",
    "brics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Hacker Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max function\n",
    "step = 0\n",
    "\n",
    "#we don't want the step to go below 0, so we chose the max number between step-1 and 0\n",
    "step = max(step - 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list ```random_walk = [0, 1, 2, 6, 3]``` and we want to visualize it.\n",
    "\n",
    "If we pass only one argument ```plt.plot(random_walk)```, Python will know what to do and will use the index of the list to map onto the x axis, and the values in the list onto the y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose np_aw: np_aw_t\n",
    "all_walks = [0, 3, 7, 10]\n",
    "np_aw = np.array(all_walks)\n",
    "\n",
    "#In this case we had to transope our array to make it look better on a plot\n",
    "np_aw_t = np.transpose(np_aw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is built on top of NumPy and Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"homelessness.csv\")\n",
    "\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.shape\n",
    "df.describe() #summary statistics for numeric columns \n",
    "df.values #data values in a 2d numpy array\n",
    "df.columns #columns name\n",
    "df.index #row number or row names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"state_pop\") #sorting rows by a certain columns\n",
    "\n",
    "df.sort_values(\"state_pop\", ascending = False) #by default it is set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"state_pop\", \"family_members\"], ascending=[True, False]) \n",
    "#firstly it sorts by state_pop and then by family_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just an example\n",
    "brics[(brics['area'] > 8) & (brics['area'] < 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .isin() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To subset to only certain values in a single column\n",
    "#instead of using & operator we can use .isin() method\n",
    "df[df[\"state\"].isin([\"Alabama\", \"Alaska\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales = pd.read_csv(\"sales.csv\", index_col=0)\n",
    "sales[\"fuel_price_usd_per_l\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct30(column):\n",
    "    return column.quantile(0.3)\n",
    "\n",
    "#The idea of agg is to apply a function to a column which will return single value\n",
    "sales[\"fuel_price_usd_per_l\"].agg(pct30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[\"fuel_price_usd_per_l\"].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.drop_duplicates(subset=\"store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(sales[\"type\"].value_counts(normalize=True),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[\"type\"].value_counts(sort = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.groupby(\"type\")[\"weekly_sales\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.groupby(\"type\")[\"weekly_sales\"].agg([\"min\", \"max\", \"sum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works pretty much the same as groupby\n",
    "sales.pivot_table(values=\"weekly_sales\", index=\"type\") #by default it calculates the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(sales.pivot_table(values=\"weekly_sales\", index=\"type\", columns=\"store\",\n",
    "                   fill_value=0, margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and Indexing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame pretty much contains out of numpy array for data and two indexes to store row and column details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To set a new index\n",
    "new_brics = brics.set_index(\"country\")\n",
    "new_brics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To reset an index\n",
    "new_brics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To completely drop an index\n",
    "new_brics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to deal with index, as it can be very helpful when it comes to subsetting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_brics.loc[[\"Russia\", \"Brazil\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_brics.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have revised loc and iloc, however i didn't find anything that would seem unusual to me, so i haven't write anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_brics = brics.pivot_table(index=\"capital\", columns=\"country\",\n",
    "                                 values=\"area\")\n",
    "pivot_brics.mean(axis=\"index\") #default value (calculates across row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "temperatures = pd.read_csv(\"temperatures.csv\", index_col=0)\n",
    "\n",
    "temperatures[\"avg_temp_c\"].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_short = temperatures[temperatures[\"country\"].isin([\"China\", \"Russia\", \"Brazil\"])]\n",
    "temperatures_grouped = temperatures_short.groupby(\"country\")[\"avg_temp_c\"].mean()\n",
    "\n",
    "temperatures_grouped.plot(kind=\"bar\", title=\"Temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copies from a lecture, do not run\n",
    "#Line plot \n",
    "df.plot(x=\"date\", y=\"weight_kg\", kind=\"line\", rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copies from a lecture, do not run\n",
    "#Scatter plot \n",
    "df.plot(x=\"height_cm\", y=\"weight_kg\", kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copies from a lecture, do not run - multiple graphs on one plot\n",
    "\n",
    "dog_pack[dog_pack[\"sex\"]==\"F\"][\"height_cm\"].hist(alpha=0.7)\n",
    "dog_pack[dog_pack[\"sex\"]==\"M\"][\"height_cm\"].hist(alpha=0.7)\n",
    "\n",
    "plt.legend([\"F\", \"M\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "brics = pd.read_csv(\"brics.csv\", index_col=0)\n",
    "brics.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = [\n",
    "    {\"name\": \"Ginger\", \"breed\": \"Dachshund\", \"height_cm\": 22,\n",
    "     \"weight_kg\": 10, \"date_of_birth\": \"2019-03-14\"},\n",
    "    {\"name\": \"Scout\", \"breed\": \"Dalmatian\", \"height_cm\": 59,\n",
    "     \"weight_kg\": 25, \"date_of_birth\": \"2019-05-09\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(list_of_dicts)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_lists={\n",
    "    \"name\": [\"Ginger\", \"Scout\"],\n",
    "    \"breed\": [\"Dachshund\", \"Dalmatian\"],\n",
    "    \"height_cm\": [22, 59],\n",
    "    \"weight_kg\": [10, 25],\n",
    "    \"date_of_birth\": [\"2019-03-14\", \"2019-05-09\"],\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(dict_of_lists)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics = pd.read_csv(\"brics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics.to_csv(\"new_brics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Data with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of data - Numeric and Categorical\n",
    "\n",
    "Numeric data can by either continouos (measured) or discrete (counted)\n",
    "\n",
    "Catgorical can be either nominal(unordered) or ordinal(ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "food = pd.read_csv(\"food.csv\")\n",
    "\n",
    "np.mean(food['consumption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(food['consumption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode - the most frequent value\n",
    "\n",
    "food[\"country\"].value_counts()\n",
    "\n",
    "#OR\n",
    "\n",
    "import statistics\n",
    "statistics.mode(food[\"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean is much more senstitive to the extreme values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(food[\"consumption\"], ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.var(food[\"consumption\"], ddof=1))\n",
    "\n",
    "#OR\n",
    "\n",
    "np.std(food[\"consumption\"], ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(food[\"consumption\"], 0.5)\n",
    "np.quantile(food[\"consumption\"], [0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "#We will get the same result if we use:\n",
    "np.quantile(food[\"consumption\"], np.linspace(0, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.boxplot(food[\"consumption\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR\n",
    "\n",
    "The distance between 25th and 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.quantile(food[\"consumption\"], 0.75) - np.quantile(food[\"consumption\"], 0.25))\n",
    "\n",
    "# OR \n",
    "from scipy.stats import iqr\n",
    "iqr(food[\"consumption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper cutoffs for outliers\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total co2_emission per country: emissions_by_country\n",
    "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
    "\n",
    "# Compute the first and third quantiles and IQR of emissions_by_country\n",
    "q1 = np.quantile(emissions_by_country, 0.25)\n",
    "q3 = np.quantile(emissions_by_country, 0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate the lower and upper cutoffs for outliers\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr\n",
    "\n",
    "# Subset emissions_by_country to find outliers\n",
    "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Numbers and Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "brics = pd.read_csv(\"brics.csv\")\n",
    "\n",
    "brics[[\"country\"]].sample() #choses a random element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20) #by setting the seed we gurantee that we will always get the same result\n",
    "brics[[\"country\"]].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brics[[\"country\"]].sample(2) #this give us two random elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samplings can be either with replacement (if we replace, then we always have the same chances like that the person will be picked for the job) or without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to allow samplings with replacement we should set replace to True\n",
    "brics[[\"country\"]].sample(3, replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Events can be either dependant (when the outcome of the first one influences the second one) or independant (when there is no influene between events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1, 7, 7) #return a list of 7 numbers between 1 and 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Law of large numbers*** - as the size of your sample increases, the sample mean will approach the expected value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected value of the distribution is the mean that is the sample is expected to have at large sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the probability that we will have to wait for the bus 7 minutes\n",
    "#Given that it arrives every 12 minutes and that we have a uniform distribution\n",
    "from scipy.stats import uniform\n",
    "less_than_seven = uniform.cdf(7, 0, 12)\n",
    "print(round(less_than_seven, 2))\n",
    "\n",
    "#Waiting more that 7 minutes\n",
    "more_than_seven = 1 - uniform.cdf(7, 0, 12)\n",
    "print(round(more_than_seven, 2))\n",
    "\n",
    "#Waiting from 4 to 7 minutes\n",
    "greater4_less7 = uniform.cdf(7, 0, 12) - uniform.cdf(4, 0, 12) \n",
    "round(greater4_less7, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "uniform.rvs(0, 5, size=10) \n",
    "#generating 10 random values between 0 and 5 according to the uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "#we throw a coin - 1 is head 0 is tails\n",
    "\n",
    "#Fliping 1 coin with a 50% probability of getting heads and we flip it 1 time only\n",
    "print(binom.rvs(1, 0.5, size=1))\n",
    "\n",
    "#Flipiing 1 coin 8 times - we get an array of values\n",
    "print(binom.rvs(1, 0.5, size=8))\n",
    "\n",
    "#we flip 8 coins one time and as a reult we get a single number, which represents the number of heads we got\n",
    "print(binom.rvs(8, 0.5, size=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the prob that we will get EXACTLY 7 heads out of 10 flips with a 0.5 chance of getting a head\n",
    "binom.pmf(7, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prob of getting LESS THAN OR EQUAL TO 7 heads out of ten\n",
    "binom.cdf(7, 10, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected value is equal to n * p.\n",
    "\n",
    "Remember that in binomial distribution all trials must be independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Distributions and the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the normal distribution has a mean = 0 and std = 1 it is a special distribution called standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What percent of women are shorter than 154 cm?\n",
    "\n",
    "from scipy.stats import norm\n",
    "norm.cdf(154, 161, 7) #we specify the needed value, mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What percent of women are taller than 154 cm?\n",
    "\n",
    "from scipy.stats import norm\n",
    "1 - norm.cdf(154, 161, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate percentiles\n",
    "## What height are 90% of women shorter than?\n",
    "\n",
    "norm.ppf(0.9, 161, 7) #we specify the percentile, mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating random numbers\n",
    "\n",
    "norm.rvs(161, 7, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling distribution of a statistic becomes closer to the normal distribution as the number of trials increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we work with a DataFrame we should use:\n",
    "df[\"column\"].hist()\n",
    "\n",
    "#if we work with a list or a numpy array:\n",
    "plt.hist(list)\n",
    "\n",
    "#This is becaues pandas is built on top of matplotlib and has inheritaded some of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The poisson distribution (discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the average number of adoptions per week is 8, what is the probaability it is going to equal 5\n",
    "#equal 5\n",
    "from scipy.stats import poisson\n",
    "\n",
    "poisson.pmf(5, 8)\n",
    "\n",
    "#Less than 5\n",
    "poisson.cdf(5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling with lambda=8\n",
    "\n",
    "poisson.rvs(8, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential distribution (continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of time between Poisson events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda (expected value) in the case of Poisson signifies number of events per minute - if ticket is bought every two minutes, the lambda will be equal to 0.5\n",
    "\n",
    "As for the expected value of Exponential distribution it can be calculated by dividing 1 by lambda so in our example it wiil be 1/0.5 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "expon.cdf(1, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distribution (continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is similar to a normal distribution. however the tails are thicker (there are more observations there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Experimental Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "freedom = pd.read_csv(\"freedom.csv\")\n",
    "sns.scatterplot(x=\"gdp_per_cap\", y=\"life_exp\", data = freedom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freedom = pd.read_csv(\"freedom.csv\")\n",
    "sns.lmplot(x=\"gdp_per_cap\", y=\"life_exp\", data = freedom, ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7019547642148015"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating correlation\n",
    "freedom[\"gdp_per_cap\"].corr(freedom[\"life_exp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important not to forget to visualize your data as sometimes correlation can't be trusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea is to make relatioship between variables more linear\n",
    "freedom[\"log_life_exp\"] = np.log(freedom[\"life_exp\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
